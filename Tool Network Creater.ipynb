{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,LeakyReLU, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "########### Load Data ###########\n",
    "#X=Trainingdata, Y=Labels\n",
    "X= pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_globalnorm_PMT160andLAPPD5x5_120k_Files_mitTopBottom.pickle\",\"rb\"))\n",
    "Y= pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_globalnorm_PMT160andLAPPD5x5_120k_Files_mitTopBottom.pickle\",\"rb\"))\n",
    "##################################\n",
    "\n",
    "#CNN Network Paramters\n",
    "dense_layer = 1        # How many Dense layers?\n",
    "nodes=512              # Nodes per dense layer\n",
    "conv_layer = 2 #4      # How many double Conv layers? \n",
    "layer_size = 100 #400  # Conv filters per layer\n",
    "filter_size = (3,3)    # Filter sizes\n",
    "\n",
    "b_s= 100               # batch_size\n",
    "ep = 2                 # epochs\n",
    "\n",
    "Label_1 = \"Electron\"\n",
    "Label_2 = \"Muon\"\n",
    "\n",
    "NAME =\"CNN-{}-filter_size-{}-double_conv-{}-nodes-{}-dense-{}\".format(filter_size,conv_layer, layer_size, dense_layer,int(time.time()))\n",
    "\n",
    "#Data will be shuffeld. Safe shuffeld sets?\n",
    "safe_suffeld = True    #will be safed with date and time\n",
    "path = 'pickle/'\n",
    "################################# \n",
    "#Should work from here on automatacally\n",
    "################################# \n",
    "unique, counts = np.unique(Y, return_counts=True, axis=0)\n",
    "print(\"How much from each label? \",counts)\n",
    "########### Shuffle data #########\n",
    "training_data = list(zip(X, Y))\n",
    "random.shuffle(training_data)\n",
    "########### Seperate data ########\n",
    "a=len(training_data)\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "for x in training_data[:int(a*0.7)]:  \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])  \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "for x in training_data[int(a*0.7):int(a*0.85)]: \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])  \n",
    "XVal = np.array(X2)\n",
    "YVal = np.array(Y2)\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "for x in training_data[int(a*0.85):]:\n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)\n",
    "del X,X1,X2,X3,Y,Y1,Y2,Y3\n",
    "\n",
    "if safe_suffeld == True:\n",
    "    now = datetime.now()\n",
    "    pickle_out = open(path+\"X_Training_{}.pickle\".format(now.strftime(\"%m.%d.%Y\")),\"wb\")\n",
    "    pickle.dump(XTraining,pickle_out,protocol=4)\n",
    "    pickle_out.close()\n",
    "    pickle_out = open(path+\"X_Val_{}.pickle\".format(now.strftime(\"%m.%d.%Y\")),\"wb\")\n",
    "    pickle.dump(XVal,pickle_out,protocol=4)\n",
    "    pickle_out.close()\n",
    "    pickle_out = open(path+\"X_Test_{}.pickle\".format(now.strftime(\"%m.%d.%Y\")),\"wb\")\n",
    "    pickle.dump(XTest,pickle_out,protocol=4)\n",
    "    pickle_out.close()\n",
    "\n",
    "########## Network ################\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(layer_size,filter_size,strides=1, input_shape= XTraining.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "model.add(Conv2D(layer_size,filter_size,padding='same',activation=\"relu\"))    \n",
    "model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "for l in range(conv_layer-1):                   \n",
    "    model.add(Conv2D(layer_size,filter_size,padding='same',activation=\"relu\"))\n",
    "    model.add(Conv2D(layer_size,filter_size,padding='same',activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))            \n",
    "#model.add(GlobalAveragePooling2D())\n",
    "model.add(Flatten())\n",
    "for l in range(dense_layer-1):\n",
    "    model.add(Dense(nodes,activation=\"relu\" ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "#model.add(Dense(32,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "#adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"adam\",\n",
    "              metrics=['accuracy']\n",
    "             )   \n",
    "filepath=NAME+\".model\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "model.summary()\n",
    "history=model.fit(\n",
    "    XTraining,YTraining,\n",
    " validation_data=(XVal,YVal)   \n",
    ",batch_size=b_s,\n",
    "shuffle=True,\n",
    "class_weight='balanced',\n",
    "callbacks=[\n",
    "            #monitor,\n",
    "            checkpoint,\n",
    "            #tensorboard \n",
    "],\n",
    "epochs= ep)\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"################################### \\n \\n Finished epochs \\n \\n ###################################\")\n",
    "print(\"Testing\")\n",
    "model = tf.keras.models.load_model(NAME+\".model\")\n",
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score (loss): ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])\n",
    "rounded_labels =np.argmax(YTest, axis=1)\n",
    "y_prob = np.array(model.predict(XTest, batch_size=128, verbose=0))\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "cm = confusion_matrix(rounded_labels, y_classes)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    " \n",
    "    print(cm)\n",
    " \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    " \n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "# Reshape into 2 x 2 matrix\n",
    "cm = cm.reshape((2,2))\n",
    " \n",
    "class_names = [Label_1, Label_2]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n CNN-model with {}% accuracy'.format(round(score[1]*100),2))\n",
    "f.savefig(\"Confusion-Matrix_\"+NAME+\".pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
